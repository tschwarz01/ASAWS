{
	"name": "02-Staging Data Loads",
	"properties": {
		"folder": {
			"name": "Day 01 - Dedicated Pool"
		},
		"content": {
			"query": "--IMPORTANT READ FIRST:\n--To run block by block, highlight the blocks/line and press Shift + Enter on your keyboard OR click on Run button at the top.\n--Run each step at once, wait till completion, review its result and proceed to the next step.\n\n--connect to your dedicated SQL pool\n\nDROP USER IF EXISTS [DWLoader]\n--STEP 1: Create SQL user\nCREATE USER DWLoader WITHOUT LOGIN;\n\n\n--STEP 2: Grant permission to the SQL User\n--TODO: Please change the \"sqlpool\" name accordingly to reflect your actual dedicated sql pool name\nGRANT CONTROL ON DATABASE::pool01 TO DWLoader;\n\n\n--STEP 3: Assign large resource grant to SQL user created\nEXEC sp_addrolemember 'largerc', 'DWLoader';\n\n\nDROP TABLE IF EXISTS [dbo].[StagingTrips];\n--STEP 4: Create heap table for staging load\nCREATE TABLE StagingTrips (\n\t[vendorID] varchar(100),\t\n\t[tpepPickupDateTime] datetime2(7),\n\t[tpepDropoffDateTime] datetime2(7),\n\t[passengerCount] int,\n\t[tripDistance] float,\n\t[puLocationId] varchar(100),\n\t[doLocationId] varchar(100),\n\t[startLon] float,\n\t[startLat] float,\n\t[endLon] float,\n\t[endLat] float,\n\t[rateCodeId] int,\n\t[storeAndFwdFlag] varchar(100),\n\t[paymentType] varchar(100),\n\t[fareAmount] float,\n\t[extra] float,\n\t[mtaTax] float,\n\t[improvementSurcharge] varchar(100),\n\t[tipAmount] float,\n\t[tollsAmount] float,\n\t[totalAmount] float\n\t)\n\tWITH (HEAP, DISTRIBUTION = ROUND_ROBIN);\n\n\n--STEP 5: Run COPY INTO statement to load public New York Taxi data from public Azure Data Lake account into heap staging table\n--TODO (IMPORTANT): Highlight the whole code block EXECUTE AS... until REVERT;\nEXECUTE AS USER = 'DWLoader'\nCOPY INTO [dbo].[StagingTrips]\nFROM \n'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/puYear=2016/puMonth=*/*.parquet',\n'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/puYear=2017/puMonth=*/*.parquet',\n'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/puYear=2018/puMonth=*/*.parquet'\nWITH\n(\n    FILE_TYPE = 'PARQUET'\n\t ,MAXERRORS = 0\n    ,IDENTITY_INSERT = 'OFF'\n)\nOPTION (LABEL = 'CopyIntoStagingTrips')\nREVERT;\n--Notice how long the data loading process takes! Keep in mind, the task loads 300M+ rows.\n\n\n--STEP 6: Check row count of the table\nSELECT COUNT(1) as 'RowCount' \nFROM dbo.StagingTrips\n--Isn't thats a big table? \n\n\n--STEP 7: Query how many parquet files were loaded and the Data Movement Service (DMS) that reads the data and writes them into SQL Pool table. \n--TODO: Upon running the query, in the result grid, click on Messages to verify the number of files loaded (720 records indicates the number of files loaded).\nSELECT DISTINCT ew.*, r.command, r.group_name\nFROM[sys].[dm_pdw_dms_external_work] ew \nJOIN sys.dm_pdw_exec_requests r \nON r.request_id = ew.request_id\nJOIN Sys.dm_pdw_request_steps s\nON r.request_id = s.request_id\nWHERE r.[label] = 'CopyIntoStagingTrips'\nORDER BY  input_name, dms_step_index\n--OPTIONAL: Expand the input_name column to review its source directory where the parquet files where loaded from within each Azure Data Lake folder\n\n\n--STEP 8: Load staged data into a hash distributed Columnstore Index table\n--TODO (IMPORTANT): Highlight the whole code block EXECUTE AS... until REVERT;\nEXECUTE AS USER = 'DWLoader'\nCREATE TABLE Trips\nWITH\n(\n\tCLUSTERED COLUMNSTORE INDEX,\n\tDISTRIBUTION = HASH([PULocationID])\n)\nAS\nSELECT * \nFROM StagingTrips\nOPTION( LABEL = 'CTAS: TripsHash')\nREVERT;\n\n--Do note that when loading data into a Clustered Columnstore Index, the data loading task also compresses the data into row group at the same time. \n--OPTIONAL: Run the following SQL query block that show many rows are compressed and how many row groups that are open in the Delta store.\nSELECT \n  rg.[state_desc] AS [RowGroup State], \n  COUNT(1) 'RowGroup Count' \nFROM \n  sys.[schemas] sm \n  JOIN sys.[tables] tb ON sm.[schema_id] = tb.[schema_id] \n  JOIN sys.[pdw_table_mappings] mp ON tb.[object_id] = mp.[object_id] \n  JOIN sys.[pdw_nodes_tables] nt ON nt.[name] = mp.[physical_name] \n  JOIN sys.[dm_pdw_nodes_db_column_store_row_group_physical_stats] rg ON rg.[object_id] = nt.[object_id] \n  AND rg.[pdw_node_id] = nt.[pdw_node_id] \n  AND rg.[distribution_id] = nt.[distribution_id] \nWHERE \n  tb.[name] = 'Trips' \nGROUP BY \n  rg.[state_desc]\n\n\n\n--STEP 9: Check row count of the CCI table\nSELECT COUNT(1) as 'RowCount' \nFROM dbo.Trips\n\n\n--STEP 10: Query how many readers and writers that \nSELECT dw.* \nFROM sys.dm_pdw_dms_workers dw\nJOIN sys.dm_pdw_exec_requests r \nON r.request_id = dw.request_id\nWHERE r.[label] = 'CTAS: TripsHash'\n--READ UPON RUNNING THE QUERY ABOVE:\n--Review how many WRITER (type column) write the rows into the table. If the bytes_processed = 0, it would mean that the writer is not writing data. \n--Ideally, when loading data into a hash table, it is recommended to have at least 60 unique values in the hash column and has very fewer NULLs (avoid if possible) otherwise, it would cause data skewness.\n--EXTERNAL_READER = Each line is an individual thread reading data from files\n--HASH_CONVERTER = Thread responsible for converting the data generated by the EXTERNAL_READER\n--WRITER = Thread responsible for writing to the correct distribution based on sink table definition. \n\n\n--IMPORTANT READ BEFORE RUNNING THE FOLLOWING QUERY: \n--Please change the connection to the \"master\" database in the \"Use database\" input at the top. \n--STEP 11: Run the following query to scale the DWU back to 100c. \n--ALTER DATABASE sqlpool --you may need to change the database name accordingly\n--MODIFY (SERVICE_OBJECTIVE = 'DW100c');",
			"metadata": {
				"language": "sql"
			},
			"currentConnection": {
				"databaseName": "master",
				"poolName": "Built-in"
			},
			"resultLimit": 5000
		},
		"type": "SqlQuery"
	}
}